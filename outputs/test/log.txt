Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-08, adam_epslin=1e-08, cache_dir='./../pretrained_model', config_name='', data_dir='./test_data', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=1e-05, load_weak=False, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', mt=0, mt_alpha1=0.99, mt_alpha2=0.995, mt_avg='exponential', mt_beta=10, mt_class='kl', mt_lambda=1, mt_loss_type='logits', mt_rampup=300, mt_updatefreq=1, n_gpu=0, no_cuda=False, num_train_epochs=100.0, output_dir='./outputs/test', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=16, remove_labels_from_weak=False, rep_train_against_weak=1, save_steps=100000, seed=42, server_ip='', server_port='', test='test', tokenizer_name='', vat=0, vat_beta=1, vat_eps=0.001, vat_lambda=1, vat_loss_type='logits', warmup_steps=100, weight_decay=0.0001)
***** Running training *****
  Num examples = 5
  Num Epochs = 100
  Instantaneous batch size per GPU = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 100
